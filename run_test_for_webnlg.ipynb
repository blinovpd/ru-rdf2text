{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0';\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import codecs\n",
    "import json\n",
    "from collections import Counter\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = []\n",
    "for data_dir in ['ru/train', 'ru/dev']:\n",
    "    for d in os.listdir(data_dir):\n",
    "        d = os.path.join(data_dir, d)\n",
    "        files = os.listdir(d)\n",
    "        for f in files:\n",
    "            f = os.path.join(d, f)\n",
    "            root = ET.parse(f)#.getroot()\n",
    "\n",
    "            for e in root.iterfind('./entries/entry'):\n",
    "                category = e.get('category')\n",
    "\n",
    "                triple_list = []\n",
    "                for mtriple in e.findall('./modifiedtripleset/mtriple'):\n",
    "                    txt = mtriple.text\n",
    "                    triple_list.append(txt)\n",
    "                    parts = txt.split('|')\n",
    "                    assert len(parts)==3\n",
    "\n",
    "                    predicates.append(parts[1].strip())\n",
    "print (len(predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_translation = set(predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.parse(\n",
    "    'ru/rdf-to-text-generation-test-data-without-refs-ru.xml'\n",
    ")\n",
    "\n",
    "for e in root.iterfind('./entries/entry'):\n",
    "    category = e.get('category')\n",
    "\n",
    "    triple_list = []\n",
    "    for mtriple in e.findall('./modifiedtripleset/mtriple'):\n",
    "        txt = mtriple.text\n",
    "        triple_list.append(txt)\n",
    "        parts = txt.split('|')\n",
    "        assert len(parts)==3\n",
    "\n",
    "        predicate = parts[1].strip()\n",
    "        predicates.append(parts[1].strip())\n",
    "        if not predicate in with_translation:\n",
    "            print (predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = dict(Counter(predicates))\n",
    "print (len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sorted(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('ru/ru_predicates.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().lower()\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicates to translate version\n",
    "predicate2translate = {}\n",
    "for k, l in zip(d, lines):\n",
    "    predicate2translate[k] = l\n",
    "print (len(predicate2translate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(predicate2translate, 'all_predicates.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicate2translate = joblib.load('all_predicates.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(e):\n",
    "    triple_list = []\n",
    "    for mtriple in e.findall('./modifiedtripleset/mtriple'):\n",
    "        parts = mtriple.text.split('|')\n",
    "        assert len(parts)==3\n",
    "        parts = [j.strip() for j in parts]\n",
    "        triple_list.append(parts)\n",
    "    return triple_list\n",
    "\n",
    "def extract_translation_dict(e):\n",
    "    en2ru = {}\n",
    "    for item in e.findall('./dbpedialinks/dbpedialink') + e.findall('./links/link'):\n",
    "        if item.get('direction')=='en2ru':\n",
    "            parts = item.text.split('|')\n",
    "            assert len(parts)==3\n",
    "            en = re.sub('_', ' ', parts[0].strip())\n",
    "            ru = re.sub('_', ' ', parts[-1].strip())\n",
    "            relation = parts[1].strip()\n",
    "            if relation=='sameAs':\n",
    "                en2ru[en] = ru\n",
    "    return en2ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recs = []\n",
    "for data_dir in ['ru/train']:\n",
    "    for d in os.listdir(data_dir):\n",
    "        #if not '1' in d:\n",
    "        #    continue\n",
    "        d = os.path.join(data_dir, d)\n",
    "        files = os.listdir(d)\n",
    "        for f in files:\n",
    "            f = os.path.join(d, f)\n",
    "            root = ET.parse(f)#.getroot()\n",
    "\n",
    "            for e in root.iterfind('./entries/entry'):\n",
    "                category, eid, size = e.get('category'), e.get('eid'), e.get('size')\n",
    "                idx = '_'.join([category, eid, size])\n",
    "                \n",
    "                triple_list = extract_triplets(e)\n",
    "                en2ru = extract_translation_dict(e)\n",
    "                \n",
    "                # translate triples\n",
    "                out_triples = []\n",
    "                for triple in triple_list:\n",
    "                    subject, obj = re.sub('_', ' ', triple[0]), re.sub('_', ' ', triple[-1])\n",
    "                    \n",
    "                    subject, obj = en2ru.get(subject, subject), en2ru.get(obj, obj)\n",
    "                    predicate = predicate2translate[triple[1]]\n",
    "                    out_triples.append( subject +' | '+ predicate +' | '+ obj )\n",
    "                \n",
    "                # extrac lex\n",
    "                lexs = []\n",
    "                for item in e.findall('./lex'):\n",
    "                    if item.get('lang')=='ru':\n",
    "                        lex = item.text\n",
    "                        lexs.append( lex )\n",
    "                index = np.argmax([len(l) for l in lexs])\n",
    "                #recs.append( (out_triples, lexs[index], idx) )\n",
    "                recs.append( ('\\n'.join(out_triples), lexs[index], idx, lexs) )\n",
    "print (len(recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # add additional RDFs from chinies trainslation\n",
    "# for fname in ['ru/ch2ru_dev_data.json', 'ru/ch2ru_train_data.json']:\n",
    "#     with codecs.open(fname, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "\n",
    "#             out_triples = '\\n'.join([e.strip() for e in data['ru_spo']])\n",
    "#             lex = data['ru_text'].strip()\n",
    "\n",
    "#             recs.append( (out_triples, lex, 0) )\n",
    "# print (len(recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recs, columns=['phrase', 'question', 'id', 'refs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distractor'] = np.random.permutation(df.question.values)\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phrase[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = df.id.apply(lambda s: s.split('_')[0])\n",
    "df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.tokenization import RubertaBPETokenizer\n",
    "model_dir = './ru-gpt2-medium-rdf-2-text/'\n",
    "tokenizer_args = {\n",
    "    'model_path': os.path.join(model_dir, 'vocab_50000.bpe'),\n",
    "    'vocab_size': 50048,\n",
    "}\n",
    "tokenizer = RubertaBPETokenizer(model_path=tokenizer_args['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AdamW, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "from transformers import GPT2Config\n",
    "\n",
    "class GPT2DoubleHeadsModel(GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        config.num_labels = 1\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.multiple_choice_head = SequenceSummary(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        mc_token_ids=None,\n",
    "        lm_labels=None,\n",
    "        mc_labels=None,\n",
    "    ):\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n",
    "\n",
    "        outputs = (lm_logits, mc_logits) + transformer_outputs[1:]\n",
    "        if mc_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        if lm_labels is not None:\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = lm_labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_len = 20\n",
    "max_tok4ph, max_tok4q = 254, 254\n",
    "pad_len = 512\n",
    "\n",
    "def encode_pair(rdf, distractor, question):\n",
    "    join_list = [[0]]\n",
    "    rset = rdf.split('\\n')\n",
    "    for triplet in rset:\n",
    "        parts = triplet.split(' | ')\n",
    "        for part in parts:\n",
    "            tokens = tokenizer.EncodeAsIds(part).tokenization[:ph_len]\n",
    "            join_list.append(tokens)\n",
    "    \n",
    "    join_list = join_list[:max_tok4ph]\n",
    "    join_list.append([50005])\n",
    "    \n",
    "    join_list.append(\n",
    "        tokenizer.EncodeAsIds(distractor).tokenization[:max_tok4q]+[2]\n",
    "    )\n",
    "    \n",
    "    seq = list(itertools.chain.from_iterable(join_list))\n",
    "    \n",
    "    mc_token_wrong = len(seq)\n",
    "    pad = pad_len-mc_token_wrong\n",
    "    pad = pad*[1]\n",
    "    seq_wrong = seq+pad\n",
    "    if len(seq_wrong)>pad_len:\n",
    "        return None\n",
    "    lm_l_wrong = len(seq_wrong)*[-100]\n",
    "    \n",
    "    start = seq_wrong.index(50005)\n",
    "    token_type_wrong = len(seq_wrong)*[1]\n",
    "    token_type_wrong[0:start] = [50010]*start\n",
    "    end = seq_wrong.index(2)+1\n",
    "    token_type_wrong[start:end] = [50005]*(end-start)\n",
    "    \n",
    "    ######## encode correct question\n",
    "    join_list = [[0]]\n",
    "    rset = rdf.split('\\n')\n",
    "    for triplet in rset:\n",
    "        parts = triplet.split(' | ')\n",
    "        for part in parts:\n",
    "            tokens = tokenizer.EncodeAsIds(part).tokenization[:ph_len]\n",
    "            join_list.append(tokens)\n",
    "            #join_list.append([50006])\n",
    "        #join_list[-1] = [50007]\n",
    "    \n",
    "    join_list = join_list[:max_tok4ph]\n",
    "    join_list.append([50005])\n",
    "    join_list.append(\n",
    "        tokenizer.EncodeAsIds(question).tokenization[:max_tok4q]+[2]\n",
    "    )\n",
    "    seq = list(itertools.chain.from_iterable(join_list))\n",
    "    \n",
    "    mc_token = len(seq)\n",
    "    pad = pad_len-mc_token\n",
    "    pad = pad*[1]\n",
    "    seq = seq+pad\n",
    "    if len(seq)>pad_len:\n",
    "        return None\n",
    "    lm_l = len(seq)*[-100]\n",
    "    start, end = seq.index(50005)+1, seq.index(2)+1\n",
    "    lm_l[start:end] = seq[start:end]\n",
    "    \n",
    "    start = seq.index(50005)\n",
    "    token_type = len(seq)*[1]\n",
    "    token_type[0:start] = [50010]*start\n",
    "    end = seq.index(2)+1\n",
    "    token_type[start:end] = [50005]*(end-start)\n",
    "    \n",
    "    input_ids = [seq_wrong, seq]\n",
    "    mc_token_ids = [mc_token_wrong-2, mc_token-2]\n",
    "    mc_labels = 1\n",
    "    lm_labels = [lm_l_wrong, lm_l]\n",
    "    token_type_ids = [token_type_wrong, token_type]\n",
    "        \n",
    "    tup = ([input_ids], [mc_token_ids], [lm_labels], [mc_labels], [token_type_ids])\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_datasets = []\n",
    "for i in range(df.shape[0]):\n",
    "    rdf = df.phrase.values[i]\n",
    "    distractor = df.distractor.values[i]\n",
    "    q = df.question.values[i]\n",
    "    tup = encode_pair(rdf, distractor, q)\n",
    "    all_datasets.append(tup)\n",
    "print (len(all_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# epochs = 3\n",
    "# lr = 3e-5\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# train_index, test_index = list(range(len(all_datasets))), list(range(4))\n",
    "\n",
    "# train_index, test_index = set(train_index), set(test_index)\n",
    "# config = GPT2Config.from_pretrained('gpt2-medium')\n",
    "# config.vocab_size = 50048\n",
    "# config.output_hidden_states = True\n",
    "\n",
    "# model = GPT2DoubleHeadsModel(config)\n",
    "# # ch2ru triplet model\n",
    "# model = model.from_pretrained( 'ru_gpt2', output_hidden_states=True )\n",
    "# model.to(device)\n",
    "\n",
    "# tensor_datasets = [e for i, e in enumerate(all_datasets) if i in train_index]\n",
    "# tensor_datasets_val = [e for i, e in enumerate(all_datasets) if i in test_index]\n",
    "\n",
    "# train_dataset = TensorDataset(\n",
    "#     torch.tensor([e[0] for e in tensor_datasets]),\n",
    "#     torch.tensor([e[1] for e in tensor_datasets]),\n",
    "#     torch.tensor([e[2] for e in tensor_datasets]),\n",
    "#     torch.tensor([e[3] for e in tensor_datasets]),\n",
    "#     torch.tensor([e[4] for e in tensor_datasets])\n",
    "# )\n",
    "# valid_dataset = TensorDataset(\n",
    "#     torch.tensor([e[0] for e in tensor_datasets_val]),\n",
    "#     torch.tensor([e[1] for e in tensor_datasets_val]),\n",
    "#     torch.tensor([e[2] for e in tensor_datasets_val]),\n",
    "#     torch.tensor([e[3] for e in tensor_datasets_val]),\n",
    "#     torch.tensor([e[4] for e in tensor_datasets_val])\n",
    "# )\n",
    "# print (len(train_dataset), len(valid_dataset))\n",
    "\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "# prediction_sampler = SequentialSampler(valid_dataset)\n",
    "# prediction_dataloader = DataLoader(valid_dataset, sampler=prediction_sampler, batch_size=batch_size*2, num_workers=4)\n",
    "\n",
    "# model = model.cuda()\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'gamma', 'beta']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "# ]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
    "\n",
    "# lm_coef, mc_coef = 1., 1.\n",
    "\n",
    "# train_loss = []\n",
    "# for _ in range(epochs):\n",
    "#     model.train(); torch.cuda.empty_cache()\n",
    "#     # Tracking variables\n",
    "#     tr_loss = 0\n",
    "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "#     # Train the data for one epoch\n",
    "#     for step, batch in enumerate(train_dataloader, start=1):\n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "#         input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         lm_loss, mc_loss, *__ = model(\n",
    "#             input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "#             mc_labels=mc_labels, lm_labels=lm_labels\n",
    "#         )\n",
    "#         loss = (lm_loss * lm_coef + mc_loss * mc_coef)\n",
    "#         train_loss.append(loss.item())\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         # Update parameters and take a step using the computed gradient\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Update tracking variables\n",
    "#         tr_loss += loss.item()\n",
    "#         nb_tr_examples += input_ids.size(0)\n",
    "#         nb_tr_steps += 1\n",
    "#         if step%100==0:\n",
    "#             print (step, tr_loss/nb_tr_steps)\n",
    "#     print ( 'epoch {} Train loss: {}'.format(_, tr_loss/nb_tr_steps) )\n",
    "\n",
    "#     ### val\n",
    "#     model.eval()\n",
    "#     # Tracking variables \n",
    "#     tr_loss, nb_tr_steps = 0, 0\n",
    "#     for step, batch in enumerate(prediction_dataloader, start=1):\n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "#         # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "#         with torch.no_grad():\n",
    "#             lm_loss, mc_loss, *__ = model(\n",
    "#                 input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "#                 mc_labels=mc_labels, lm_labels=lm_labels\n",
    "#             )\n",
    "#             loss = (lm_loss * lm_coef + mc_loss * mc_coef)\n",
    "\n",
    "#             tr_loss += loss.item()\n",
    "#             nb_tr_steps += 1\n",
    "#     print ( 'val loss: {}'.format(tr_loss/nb_tr_steps) )\n",
    "# model.train();\n",
    "# model.save_pretrained( 'ru-gpt2-medium-rdf-2-text' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2-medium') #cache_dir='/ayb/vol2/home/blinoff/.cache'\n",
    "config.vocab_size = 50048\n",
    "config.output_hidden_states = True\n",
    "\n",
    "model = GPT2DoubleHeadsModel(config)\n",
    "model = model.from_pretrained( 'ru-gpt2-medium-rdf-2-text', output_hidden_states=True )\n",
    "model.to(device)\n",
    "model.train();\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    assert logits.dim() == 1#Only work for batch size 1 for now-could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_q_set(raw_text, num_samples=17, temperature=.7, top_k=11, top_p=.9, max_lenght=64):\n",
    "    with torch.no_grad():\n",
    "        torch.manual_seed( np.random.randint(1000) )\n",
    "\n",
    "        #orig_input_ids = [0]+tokenizer.EncodeAsIds(raw_text).tokenization+[50005]\n",
    "        #orig_token_type_ids = len(orig_input_ids)*[50010]; orig_token_type_ids[-1] = 50005\n",
    "        join_list = [[0]]\n",
    "        rset = raw_text.split('\\n')\n",
    "        for triplet in rset:\n",
    "            parts = triplet.split(' | ')\n",
    "            for part in parts:\n",
    "                tokens = tokenizer.EncodeAsIds(part).tokenization[:ph_len]\n",
    "                join_list.append(tokens)\n",
    "                #join_list.append([50006])\n",
    "            #join_list[-1] = [50007]\n",
    "\n",
    "        join_list = join_list[:max_tok4ph]\n",
    "        join_list.append([50005])\n",
    "        orig_input_ids = list(itertools.chain.from_iterable(join_list))\n",
    "        orig_token_type_ids = len(orig_input_ids)*[50010]; orig_token_type_ids[-1] = 50005\n",
    "        \n",
    "        result = {}\n",
    "        for j in range(num_samples):\n",
    "            input_ids = orig_input_ids.copy()\n",
    "            token_type_ids = orig_token_type_ids.copy()\n",
    "            input_ids_prob = []\n",
    "\n",
    "            for i in range(max_lenght):\n",
    "                t_input_ids = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "                t_token_type_ids = torch.tensor(token_type_ids, device=device).unsqueeze(0)\n",
    "                #print (t_input_ids.shape)\n",
    "                #print (t_token_type_ids.shape)\n",
    "                \n",
    "                logits = model( t_input_ids, token_type_ids=t_token_type_ids )\n",
    "\n",
    "                logits = logits[0] #as it is tuple we need only 0 elem\n",
    "                #print (logits.shape)\n",
    "                #print ()\n",
    "\n",
    "                logits = logits[0, -1, :] / temperature\n",
    "                #print (logits.shape)\n",
    "                #print ()\n",
    "\n",
    "                logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                prev = torch.topk(probs, 1)[1]\n",
    "                input_ids_prob.append( probs[prev].item() )\n",
    "                \n",
    "                tok = prev.item()\n",
    "                input_ids.append( tok )\n",
    "                token_type_ids.append( 50005 )\n",
    "                if tok==2:\n",
    "                    break\n",
    "            \n",
    "            if not 2 in set(input_ids):#ad hock if there is no end\n",
    "                input_ids[-1] = 2\n",
    "            s,e = input_ids.index(50005)+1, input_ids.index(2)\n",
    "            \n",
    "            q = tokenizer.DecodeIds(input_ids[s:e])\n",
    "            if not q in result:\n",
    "                result[q] = []\n",
    "            \n",
    "            l = len(orig_input_ids)\n",
    "            p = np.prod( input_ids_prob[s-l:e-l] ) / len(input_ids_prob[s-l:e-l])\n",
    "            result[q].append(p)\n",
    "            \n",
    "        #print (result)\n",
    "        result = [k for k, v in sorted(result.items(), key=lambda item: np.max(item[1]))]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "\n",
    "f = 'ru/rdf-to-text-generation-test-data-without-refs-ru.xml'\n",
    "root = ET.parse(f)\n",
    "\n",
    "for e in root.iterfind('./entries/entry'):\n",
    "    category, eid, size = e.get('category'), e.get('eid'), e.get('size')\n",
    "    idx = '_'.join([category, eid, size])\n",
    "\n",
    "    triple_list = extract_triplets(e)\n",
    "    en2ru = extract_translation_dict(e)\n",
    "\n",
    "    # translate triples\n",
    "    out_triples = []\n",
    "    for triple in triple_list:\n",
    "        subject, obj = re.sub('_', ' ', triple[0]), re.sub('_', ' ', triple[-1])\n",
    "\n",
    "        subject, obj = en2ru.get(subject, subject), en2ru.get(obj, obj)\n",
    "        predicate = predicate2translate[triple[1]]\n",
    "        out_triples.append( subject +' | '+ predicate +' | '+ obj )\n",
    "\n",
    "    # extrac lex\n",
    "    lexs = []\n",
    "    for item in e.findall('./lex'):\n",
    "        if item.get('lang')=='ru':\n",
    "            lex = item.text\n",
    "            lexs.append( lex )\n",
    "    #index = np.argmax([len(l) for l in lexs])\n",
    "    #recs.append( (out_triples, lexs[index], idx) )\n",
    "    recs.append( ('\\n'.join(out_triples), lexs, idx) )\n",
    "print (len(recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypothesis, references = [], []\n",
    "for i, (ph, refs, idx) in enumerate(recs):\n",
    "    candidates = generate_q_set(ph, num_samples=19)\n",
    "    hypothesis.append(candidates)\n",
    "    references.append(refs)\n",
    "    print (i)\n",
    "    # if i>10:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetShiftingWindows(thelist, size=2):\n",
    "    return [ thelist[x:x+size] for x in range( len(thelist) - size + 1 ) ]\n",
    "\n",
    "class Detokenizer:\n",
    "    def __init__(self):\n",
    "        self.paired_pattern = re.compile('[\\\"“(«<{\\[].*?[\\\"”)»>}\\]]')\n",
    "        self.float_sep_pattern = re.compile('\\d*[\\.,] \\d*')\n",
    "        self.float_pattern = re.compile('(?<![a-zA-Z:])[-+]?\\d*[\\.,]\\d+')\n",
    "        \n",
    "    def translate(self, s, rdf):\n",
    "        dash_elements = []\n",
    "        lines = rdf.split('\\n')\n",
    "        float_values = []\n",
    "        for line in lines:\n",
    "            dash_elements.extend(re.findall('.-.', line))\n",
    "            parts = line.split(' | ')\n",
    "            # find float numbers in subject or object strings\n",
    "            curr_floats = self.float_pattern.findall( parts[0] )\n",
    "            float_values.extend(curr_floats)\n",
    "            curr_floats = self.float_pattern.findall( parts[-1] )\n",
    "            float_values.extend(curr_floats)\n",
    "        float_values = set([re.sub(',', '.', v) for v in float_values])\n",
    "        \n",
    "        dash_elements = set(dash_elements)\n",
    "        \n",
    "        ### normailze float values\n",
    "        points = [0]\n",
    "        for e in self.float_sep_pattern.finditer(s):\n",
    "            points.extend(e.span())\n",
    "        points.append(len(s))\n",
    "        \n",
    "        to_join = []\n",
    "        for i, (start, end) in enumerate(GetShiftingWindows(points)):\n",
    "            chunk = s[start:end]\n",
    "            if i%2:\n",
    "                replace_chunk = re.sub(',', '.', chunk)\n",
    "                replace_chunk = re.sub(' +', '', replace_chunk)\n",
    "                if replace_chunk in float_values:\n",
    "                    chunk = replace_chunk\n",
    "            to_join.append(chunk)\n",
    "        s = ''.join(to_join)\n",
    "        \n",
    "        ### collapse spaces for paired punctuations\n",
    "        points = [0]\n",
    "        for e in self.paired_pattern.finditer(s):\n",
    "            points.extend(e.span())\n",
    "        points.append(len(s))\n",
    "        \n",
    "        to_join = []\n",
    "        for i, (start, end) in enumerate(GetShiftingWindows(points)):\n",
    "            chunk = s[start:end]\n",
    "            if i%2:\n",
    "                ch_start, ch_end = chunk[0], chunk[-1]\n",
    "                chunk = chunk[1:-1].strip()\n",
    "                chunk = ch_start+chunk+ch_end\n",
    "            to_join.append(chunk)\n",
    "        res = ''.join(to_join)\n",
    "        \n",
    "        ### 'a - b' cases\n",
    "        collapse_elements = re.findall('. - .', res)\n",
    "        for e in collapse_elements:\n",
    "            wo_space_e = re.sub(' +', '', e)\n",
    "            if wo_space_e in dash_elements:\n",
    "                res = re.sub(e, wo_space_e, res)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "detok = Detokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'w') as f:\n",
    "    ending = ''\n",
    "    for i, h in enumerate(hypothesis):\n",
    "        best_candidate = h[-1]\n",
    "        rdf = recs[i][0]\n",
    "        line = detok.translate(best_candidate, rdf)\n",
    "        f.write(ending+line)\n",
    "        ending = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
